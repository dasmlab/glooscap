apiVersion: v1
kind: Namespace
metadata:
  labels:
    control-plane: controller-manager
    app.kubernetes.io/name: operator
    app.kubernetes.io/managed-by: kustomize
  name: system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: controller-manager
  namespace: system
  labels:
    control-plane: controller-manager
    app.kubernetes.io/name: operator
    app.kubernetes.io/managed-by: kustomize
spec:
  selector:
    matchLabels:
      control-plane: controller-manager
      app.kubernetes.io/name: operator
  replicas: 1
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: manager
      labels:
        control-plane: controller-manager
        app.kubernetes.io/name: operator
    spec:
      # TODO(user): Uncomment the following code to configure the nodeAffinity expression
      # according to the platforms which are supported by your solution.
      # It is considered best practice to support multiple architectures. You can
      # build your manager image using the makefile target docker-buildx.
      # affinity:
      #   nodeAffinity:
      #     requiredDuringSchedulingIgnoredDuringExecution:
      #       nodeSelectorTerms:
      #         - matchExpressions:
      #           - key: kubernetes.io/arch
      #             operator: In
      #             values:
      #               - amd64
      #               - arm64
      #               - ppc64le
      #               - s390x
      #           - key: kubernetes.io/os
      #             operator: In
      #             values:
      #               - linux
      securityContext:
        # Projects are configured by default to adhere to the "restricted" Pod Security Standards.
        # This ensures that deployments meet the highest security requirements for Kubernetes.
        # For more details, see: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      imagePullSecrets:
        - name: dasmlab-ghcr-pull
      containers:
      - command:
        - /manager
        args:
          - --leader-elect
          - --health-probe-bind-address=:8081
        image: controller:latest
        name: manager
        ports:
        - containerPort: 3000
          name: http-api
        securityContext:
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - "ALL"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8081
          initialDelaySeconds: 5
          periodSeconds: 10
        env:
        - name: GLOOSCAP_API_ADDR
          value: ":3000"
        - name: VLLM_JOB_NAMESPACE
          value: nanabush
        - name: VLLM_JOB_IMAGE
          value: quay.io/dasmlab/vllm-runner:latest
        - name: VLLM_API_URL
          value: http://vllm.nanabush.svc:8000
        # ====================================================================
        # Translation Service Configuration
        # ====================================================================
        # Glooscap supports two translation service backends:
        #   1. Nanabush - Heavy vLLM-based service (GPU required)
        #   2. Iskoces  - Lightweight MT service (CPU-only, LibreTranslate/Argos)
        #
        # Both use the same gRPC proto interface, so switching is seamless.
        # ====================================================================
        
        # --------------------------------------------------------------------
        # OPTION 1: Use Iskoces (Lightweight MT - Recommended for development)
        # --------------------------------------------------------------------
        # Uncomment these lines to use Iskoces:
        # - name: TRANSLATION_SERVICE_ADDR
        #   value: "iskoces-service.iskoces.svc:50051"  # Kubernetes service address
        #   # Or for local testing: "localhost:50051"
        # - name: TRANSLATION_SERVICE_TYPE
        #   value: "iskoces"  # Optional, auto-detected from address if contains "iskoces"
        # - name: TRANSLATION_SERVICE_SECURE
        #   value: "false"  # Set to "true" if using TLS/mTLS
        
        # --------------------------------------------------------------------
        # OPTION 2: Use Nanabush (vLLM-based - Production/GPU workloads)
        # --------------------------------------------------------------------
        # Option 2a: Use generic environment variables (recommended)
        # - name: TRANSLATION_SERVICE_ADDR
        #   value: "nanabush-service.nanabush.svc:50051"  # Kubernetes service
        #   # Or external: "209.15.95.244:50051"  # HAProxy endpoint
        # - name: TRANSLATION_SERVICE_TYPE
        #   value: "nanabush"  # Optional, auto-detected from address
        # - name: TRANSLATION_SERVICE_SECURE
        #   value: "false"
        
        # Option 2b: Use Nanabush-specific variables (backward compatible)
        # Nanabush gRPC server configuration (on ocp-sno-1050ti cluster)
        # HAProxy endpoint provides L2 TCP forwarding from ocp-ai-sno-2 cluster
        # Using external IP directly (internal DNS resolves to unreachable IP)
        - name: NANABUSH_GRPC_ADDR
          value: "209.15.95.244:50051"
        - name: NANABUSH_SECURE
          value: "false"  # Direct TCP connection, no TLS (HAProxy forwards raw TCP/HTTP/2)
        
        # ====================================================================
        # Configuration Priority:
        #   1. TRANSLATION_SERVICE_ADDR (if set, takes precedence)
        #   2. NANABUSH_GRPC_ADDR (fallback for backward compatibility)
        # ====================================================================
        # TODO(user): Configure the resources accordingly based on the project requirements.
        # More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        resources:
          limits:
            cpu: 500m
            memory: 128Mi
          requests:
            cpu: 10m
            memory: 64Mi
        volumeMounts: []
      volumes: []
      serviceAccountName: controller-manager
      terminationGracePeriodSeconds: 10
